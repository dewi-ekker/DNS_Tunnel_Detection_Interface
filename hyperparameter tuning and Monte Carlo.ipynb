{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297f26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171d1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "df = pd.read_csv('DNS_datastore.csv', index_col=0)\n",
    "features = list(df.columns)[11:]\n",
    "\n",
    "x = np.array(df.loc[:,features].fillna(0))\n",
    "y = np.array(df['Label'])\n",
    "\n",
    "sigma = (0.03 * x.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c01ed5",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning of the RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fb307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13200 candidates, totalling 66000 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid to be searched:\n",
    "    # *The choices for max depth and number of estimators are based on \n",
    "    # how much can fit in the interface while still providing clear overview.\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'n_estimators': list(range(4,15)),\n",
    "    'max_features': ['sqrt', 'log2'] + [x/10 for x in range(1,11)],\n",
    "    'max_depth': list(range(2,6)),\n",
    "    'min_samples_leaf': [x/10 for x in range(1,6)], #fraction\n",
    "    'min_samples_split': [x/10 for x in range(1,6)] #fraction  \n",
    "}\n",
    "\n",
    "# Instantiate the grid search model:\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, scoring=['precision','accuracy','f1'], \n",
    "                           refit='precision', cv=5, n_jobs = -1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the data:\n",
    "grid_search.fit(x, y)\n",
    "print('\\nThe parameters with the best f1_score are\\n')\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2551f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = grid_search.best_estimator_\n",
    "\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(rf_tuned, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b175f5b",
   "metadata": {},
   "source": [
    "#### Comparing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ae8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_scores(model, x=x, y=y, sigma=sigma):\n",
    "    acc = []\n",
    "    F1 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        noise = np.random.normal(0, sigma, size=x.shape)\n",
    "        x = x + noise\n",
    "        cv_results = cross_validate(model, x, y, cv=10, scoring=('accuracy', 'f1'))\n",
    "        acc = acc + cv_results['test_accuracy']\n",
    "        F1  = F1  + cv_results['test_f1']\n",
    "    \n",
    "    return acc, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cae751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_grid = {'bootstrap': True,\n",
    "#              'max_depth': 3,\n",
    "#              'max_features': 5,\n",
    "#              'min_samples_leaf': 0.1,\n",
    "#              'min_samples_split': 0.2,\n",
    "#              'n_estimators': 11,\n",
    "#              'n_jobs': -1,\n",
    "#              'verbose': 1}\n",
    "\n",
    "# rf_tuned = RandomForestClassifier()\n",
    "# rf_tuned.set_params(**best_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = pd.DataFrame()\n",
    "F1_scores = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest, base model\n",
    "rf_base = RandomForestClassifier()\n",
    "acc_scores['RF base'], F1_scores['RF base'] = avg_scores(rf_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Random Forest, tuned model\n",
    "#rf_tuned = grid_search.best_estimator_\n",
    "acc_scores['RF tuned'], F1_scores['RF tuned'] = avg_scores(rf_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678867c",
   "metadata": {},
   "source": [
    "compare against other algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "acc_scores['DT'], F1_scores['DT'] = avg_scores(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn import svm\n",
    "svm = svm.SVC()\n",
    "acc_scores['SVM'], F1_scores['SVM'] = avg_scores(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier()\n",
    "acc_scores['KNN'], F1_scores['KNN'] = avg_scores(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acc_scores.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f1bc5",
   "metadata": {},
   "source": [
    "plot accuracies and F1-scores in a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(acc_scores.shape[1])\n",
    "width = 0.05\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "acc_bars = ax.bar(ind-width/2, list(acc_scores.mean(axis=1)), width, yerr=list(acc_scores.std(axis=1)), label='Accuracy',\n",
    "                 error_kw=dict(capsize=5, lw=0.5, capthick=0.5), color='lightskyblue', ecolor='navy')\n",
    "F1_bars = ax.bar(ind+width/2, list(F1_scores.mean(axis=1)), width, yerr=list(F1_scores.std(axis=1)), label='F1-score')\n",
    "plt.ylim(ymin = 0.9, ymax = 1)\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(acc_scores.columns)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('Model_Metrics/Compared_Performances.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d225cac",
   "metadata": {},
   "source": [
    "#### Verification of parameters with one change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(grid, name, est=rf_tuned, x=x, y=y, sigma=sigma):\n",
    "    \n",
    "    param_name = 'param_%s' % list(grid.keys())[0]\n",
    "\n",
    "    trainscores = pd.DataFrame()\n",
    "    testscores = pd.DataFrame()\n",
    "    fittime = pd.DataFrame()\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Create a model with 10-fold cross validation\n",
    "        model = GridSearchCV(est=rf_tuned, param_grid=grid, cv=10, scoring='accuracy', return_train_score=True)\n",
    "        \n",
    "        # Add random Gaussian noise with every repetition\n",
    "        noise = np.random.normal(0, sigma, size=x.shape)\n",
    "        x = x + noise\n",
    "        model.fit(x, y)\n",
    "\n",
    "        # Extract information from the cross validation model\n",
    "        trainscores.loc[:,i] = model.cv_results_['mean_train_score']\n",
    "        testscores.loc[:,i] = model.cv_results_['mean_test_score']\n",
    "        fittime.loc[:,i] = model.cv_results_['mean_fit_time']\n",
    "    \n",
    "    train_scores = trainscores.mean(axis=1)\n",
    "    train_std    = trainscores.std(axis=1)\n",
    "    \n",
    "    test_scores  = testscores.mean(axis=1)\n",
    "    test_std     = testscores.std(axis=1)\n",
    "    \n",
    "    time_mean    = fittime.mean(axis=1)\n",
    "    time_std     = fittime.std(axis=1)\n",
    "    \n",
    "    param_values = list(model.cv_results_[param_name])\n",
    "    print(param_values)\n",
    "    \n",
    "    \n",
    "    # Plot the scores over the parameter\n",
    "    plt.subplots(1, 2, figsize=(10, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(param_values, train_scores, 'b-', label = 'train')\n",
    "    plt.fill_between(param_values, train_scores-train_std, train_scores+train_std, alpha=0.5, color='b')\n",
    "    plt.plot(param_values, test_scores, 'g-', label = 'test')\n",
    "    plt.ylim(ymin = 0.94, ymax = 1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('accuracy vs %s' % name)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(param_values, time_mean, 'r-')\n",
    "    plt.fill_between(param_values, time_mean-time_std, time_mean+time_std, alpha=0.5, color='r')\n",
    "    plt.ylim(ymin = 0.0, ymax = 1.5)\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel('Train time (sec)')\n",
    "    plt.title('Training time vs %s' % name)\n",
    "    \n",
    "    plt.tight_layout(pad = 4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa5f5997",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Grid with only the number of trees changed\n",
    "tree_grid = {'n_estimators': list(range(2,31))}\n",
    "\n",
    "plot_results(tree_grid, ' number of trees')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc7227ec",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "#grid with only the number of features changed\n",
    "feature_grid = {'max_features': list(range(1, x_train.shape[1] + 1))}\n",
    "\n",
    "plot_results(feature_grid, 'max features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82280ae",
   "metadata": {},
   "source": [
    "#### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb84b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = {'benign plain.pcap':0,\n",
    "#              'dns2tcp tunneling.pcap':1,\n",
    "#              'dnscapy tunneling.pcap':1,\n",
    "#              'iodine tunneling.pcap':1,\n",
    "#              'tuns_c_00000_20180330104021.pcap':1}\n",
    "\n",
    "# for file, label in file_list.items():\n",
    "#     print(label, '\\t' , 'data/pcaps/'+file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(rf_tuned.feature_importances_, columns=['fi'], index=features)\n",
    "importances['std'] = np.std([tree.feature_importances_ for tree in rf_tuned.estimators_], axis=0, ddof=1)\n",
    "importances.sort_values('fi', ascending=False, inplace=True)\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(rf_tuned)\n",
    "sel.fit(x_train, y_train)\n",
    "selected_feat= df.loc[:,features].columns[(sel.get_support())]\n",
    "print(list(selected_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sel.estimator_.feature_importances_.ravel()).hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
